<img style="float: left; padding:20px" src="media/clip.svg" alt="movieclip" width="75"/>

# movieclip

This repository is an experiment in contrastive learning and how it can be applied to something I like a lot - movies. What if we can train models to recognize cinematographic styles from training on movie scenes? This is what I try to solve in this experiment.

The repo is divided into 2 parts:

- [Dataset](#dataset) contains code for how the dataset is generated. The dataset is published to HuggingFace.
- [Model](#model) - contains a Jupyter notebook exported from Colab where I trained the model.

More information about these are present in their respective directories.

## Dataset

## Model
 
## Licensing

This repository is licensed under the Apache2.0 License.

## References

