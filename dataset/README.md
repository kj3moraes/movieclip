# Dataset

The MovieScene dataset is published to HuggingFace here -  

This documentation shows how to generate the dataset yourself. Feel free to edit the methodology for your own purposes.
The dataset is generated by:

1. scraping [FILM-GRAB](https://film-grab.com/)'s A-Z section and saving all the images from this website.
2. querying the TMDB database for each movie for the movie ID, specifically the director, cinematographer and genres.
3. saving all this information into a directory and JSON for the movie information.

## Setup

You need to create a .env file in this directory and populate it as follows:

```env
OMDB_API_KEY=<your-api-key>
HUGGINGFACE_API_KEY=<your-api-key>
```

You can create a TMDB account [here](https://www.themoviedb.org/) and then make an API key and read token.

If you want isolation and don't want the dependencies of this process to interfere with your dev env, I recommend using [Poetry](https://python-poetry.org/docs/basic-usage/) and running the With Poetry option under the Running section.


## Running

To run the code for this dataset generation, you need to first

1. Download the dataset images
2. Caption the images of the dataset.

### With Poetry

To run this in a virtual environment with Poetry, you can simply do 

```bash
$ poetry shell
> python3 download.py
> python3 captioning.py
```

### Without Poetry

You can install the requirements and run the 2 commands as shown above

```bash
$ pip install -r requirements.txt
$ python3 download.py
...
Successfully downloaded the dataset to the data/ directory
$ python3 captioning.py
...
Successfully captioned all the images in the data/ directory
```

If you have run the above steps to generate the dataset, you can skip the next section on downloading

## Download for Use

### S3 Download

```bash
wget s3://moviescene-dataset/moviescene__2024_01.zip 
unzip moviescene__2024_01.zip
```

## Using the Dataset

An example of using only the images from the dataset is here:

```python
class MovieclipDataset: 
    def __init__(self, path: str, patch_size=320, is_validation=False, should_normalize=True, filters: list):
        # Get the paths for the data
        dataset_path = Path(path)
        training_path = dataset_path / 'train'
        if not training_path.exists(): raise Exception(f"Dataset not found at {training_path}")
        validation_path = dataset_path / 'test'
        if not validation_path.exists(): raise Exception(f"Dataset not found at {validation_path}")
        self.folder = training_path if not is_validation else validation_path

        # Set the normalization and the validation flags
        self.is_validation = is_validation
        self.should_normalize = should_normalize

        self.images = []
        for movie_id_dir in self.folder.iterdir():
            if movie_id_dir.is_dir():
                # Check if the movie_id passes the filters. Use the 
                # images in the directory ONLY IF IT PASSES ALL OF THEM.
                if all(pass_fn(movie_id_dir.name) for pass_fn in filters):
                    for file in movie_id_dir.iterdir():
                        if file.is_file() and file.suffix == ".jpg":
                            self.images.append(subdir / file)

        self.patch_size = patch_size
        self.resize = transforms.Resize((patch_size, patch_size))
        self.normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

    def __getitem__(self, index):
        assert index > 0 and index < len(self.images)
        image_fname = self.images[index]
        image = Image.open(image_fname)
        
        # Transform the image
        image = self.resize(image)
        image_t = transforms.functional.to_tensor(image)
        if image_t.shape[0] == 1: image_t = image_t.expand(3, self.patch_size, self.patch_size)
        if self.should_normalize:
          image_t = self.normalization(image_t)

        return image_t

    def __len__(self):
        return len(self.images)
```

### How to use Filters

Filters are a key prt 

## Disclaimer

### All the images here have been gotten from FILMGRAB. They are not my images. This dataset is meant only for research and development and not for any commercial purposes.
